title: '[译文] Reflex: remote flash ≈ local flash'
date: 2018-05-13 19:10:53
categories: Theory
tags: [flash, storage, data center, QoS, IX, data plane, translation, TLDR]
---

在数据中心中的，采用NVMe远程获取能够灵活扩展和高效使用Flash利用率和IOPS。然而，现有对远程Flash的获取系统要么有较大的性能开销，要么不能够分离在分享同一个Flash设备的不同客户端。我们提出一个基于软件的远程Flash获取系统ReFlex，能够提供近乎获取本地Flash一样的性能。 ReFlex使用了一个数据平面内核来高度集成网络和存储处理，并以较少的资源要求下实现低延时和高吞吐量。ReFlex每个在TCP/IP网络核心能够提供多至850K的IOPS， 然而却仅仅只比获取本地Flash多花21us。ReFlex使用的QoS调度器能够为数千个远程客户端执行可忽略的时延和吞吐服务级别目标（SLO）。我们会展示ReFlex是如何允许应用去使用远程Flash却又保持本地Flash的性能。


<!--more-->

### 1. 简介

NVMe Flash设备以小于100us的延迟提供上至每秒100万次I/O操作（IOPS），令它们成为许多数据密集型在线服务的存储介质首选。然而，数据中心中部署的Flash设备往往未被充分利用，这是基于长时间跨应用不同的需求造成的。通常来说，为所有负载在CPU，内存和Flash资源直接去设计一个完美的平衡是非常困难的，这导致了过量供应和更高的拥有者开销（TCO）。和数据中心中的共享磁盘类似，通过网络对Flash的远程获取能够以一种允许获取要么在任何有空闲容量和带宽要么专门提供大规模NVMe设备的服务器的Flash获取的形式大幅提升利用率。

实施对远程Flash的获取具有很大的挑战性。要实现低延迟，在服务器和客户端都要求最小化网络和存储层的开销。除了低延迟，每个服务器必须以最小的开销实现高吞吐，用一小部分CPU核心来满足1个或者多个NVMe Flash设备。此外，管理多个租户共享的Flash设备之间的干涉和对Flash设备之间不平衡的读/写行为，需要考虑一种隔离机制，以确保能够提供所有租户可预测的性能。最后，拥有对用于远程连接的共享级别、部署规模和网络协议的灵活度是非常有用的。现有的对于远程Flash获取的软件选项，例如iSCSI或给予时间的服务器，不能够满足性能预期。最近提出的硬件加速的选项，像RDMA fabric之上的NVMe，缺乏性能隔离并且只能够提供有限的部署灵活性。

我们提出ReFlex，一个基于软件的Flash存储服务器，它能够以媲美本地Flash获取性能的获取远程Flash。ReFlex能以较小的计算要求，利用创新的紧密耦合网络和存储的数据平面内核来实现高性能表现。数据平面被设计成能够避免中断和数据复制的开销来最优化局部性，并且实现一个高吞吐（IOPS）和低尾时延的平衡。ReFlex包括一个实施优先级和速率限制的QoS调度器，能够为共享同一个设备的租户执行可忽略的时延和吞吐服务级别目标（SLO）。ReFlex既提供用户级别的库也提供一个远程块设备驱动器来支持客户端应用。

ReFlex服务器实现了用10GbE TCP/IP网络上平均850K 每核心的IOPS。因此，它能够服务多个NVMe设备并以低开销达到网络线速。它的未加载时延仅仅比通过NVMe队列直接获取本地Flash多21us。ReFelex服务器能够支撑上千个远程租户。它的QoS调度能够使租户的尾时延和吞吐量要求达到SLOs，并允许出力最多的租户消耗掉NVMe设备所有的剩余吞吐量。最后，采用出啊痛应用，我们展示了即使是重载客户，ReFlex也能够提供近乎本地Flash的性能级别。

ReFlex是开源软件，代码在：https://github.com/stanford-mast/reflex

### 2. 背景和动机

远程获取提供了实用Flash的灵活性而不用考虑它在数据中心的物理位置，增加使用率的同时降低了整体的拥有成本。

在数据中心中，对于硬盘的远程获取已经非常普遍，因为它们的高时延和低吞吐轻易地掩盖了忘了开销。许多软件系统能够令使远程硬盘作为块设备可用（如iSCSI），网络文件系统（如NFS），分布式文件系统（如谷歌文件系统）或是分布式数据存储（如Bigtable）。也有很多硬件加速的远程flash获取次啊用远程直接内存获取（RDMA）如（NVMe over Fabrics或PCIe互联）。现有的对于远程Flash的获取面临着两个出要的挑战：以低开销实现高性能，和为干扰的存在提供可预测的性能。

#### 2.1 性能目标

低时延：NVMe Flash的未加载读时延在20-100us。为了满足时延敏感应用的要求，对于Flash的远程获取应该有较低的实验开销。传统的在10GbE和TCP/IP网络协议远程服务器获取甚至会令在软件存储协议开始之前的加载时延增加至少50us。更重要的是，Linux的网络处理因为中断管理和核心调度引入了不可预测的表现，这增加了远程获取的wei ba时延。类似iSCSI的网络存储系统为协议处理和内核与用户缓存之间的数据复制带来了额外的时延。另一方面，像GFS和HDFS这一类分布式分布式文件系统是为多兆数据传输到远程磁盘最优化的，引入了千比特大小的数据获取会带来间接开销。

低开销的高吞吐量：现代的Flash设备拥有百万量级IOPS的吞吐能力。一个远程Flash服务器必须能能够用低处理的间接开销提供高I/O比率来减少花费并增加在网络Flash的灵活性。数据中心中嗲有多余Flash能力和IOPS的机器可能并没有许多可用的核心来处理远程的请求。现有的软件方法要求大量对资源的计算来用满Flash的吞吐量。比如说，iSCSI协议实现了每个CPU和行70K的IOPS，因此需要14个核心来支持一个一兆IOPS的高端NVMe设备。类似的，在$5中，我们展示了一个基于Linux的libevent和libaio的每个核心仅仅75IOPS的轻量级服务器来获取远程Flash。

#### 2.2 干涉管理

远程Flash有用之处在于即使在多租户共享一个设备也能够提供可预测的表现。因为读写干涉的影响，可预测的表现对于NVMe Flash设备来说从来都是个挑战。Figure 1绘制了Flash上的尾读延迟（95th 百分位数）来作为一个吞吐量（IOPS）在不同读/写率下的工作负载。尾读延迟取决于吞吐量（负载）和读写率。这个行为对于所有我们测试过的NVMe Flash设备都是特殊的，因为写操作更慢而且会触发不能被隐藏的耐久提升和垃圾回收活动。读/写干涉在单个应用使用本地Flash设备时被管理，但会在使用远程flash和多租户共享同一设备时互相之间变得不可知。

硬件加速是不够的：干涉的缓解是像NVMe over Fabric，QuickSAN或是iSCSI extensions for RDMA（iSER）硬件加速方案最主要的遗忘之处。NVMe设备现有的隔离特性，也就是硬件队列和命名空间，在不增加额外的软件下对于减少多个远程客户端的相互干涉是不够的。队列的数量是有限的（例如：拥有64个队列已经是很高端的设备）而且请求仲裁是过分简单的（round-robin）。命名空间是设备的主机端逻辑分片，所以被发往不同命名空间的请求人就会干扰。现有的NVMe over Fabrics方案并没有用I/O调度来缓解多个远程客户端之间的干涉。现有方案并没有提供一个给客户端的方法来指定服务目标的质量，所以因此不能够以QoS感知的方式来管理Flash设备。

硬件加速的方案有另一种缺点。机遇RDMA的策略需要网络结构有RDMA的能力，这对于传统数据中心并不是可以简单实现的。在PCIe总线共享Flash的做法限制其只能在单一机架上也缺乏对运行的隔离。

### 3. ReFlex 设计

ReFlex利用一个紧耦合网络和数据层的数据平面架构来提供低延迟和高吞吐量的远程Flash获取。它能在基本的如TCP/UCP网络协议为任何大小的逻辑块服务远程读/写请求。尽管主要是一个软件系统，ReFlex影响了NIC和NVMe Flash设备的硬件虚拟化能力来直接在硬件队列上操作并且有效率地在NIC和Flash设备之间不用复制地转发请求和数据。它基于轮询的执行模型让请求能够不被中断地执行，提升局部性的同时降低不可预测性。ReFlex采用了一个创新的I/O调度器来为租户保证时延和吞吐量在不同读/写请求比率下的的SLO。ReFlex能够为多达数千个租户和网络连接服务，尽可能多地利用所需要的核心来用满Flash设备的IOPS。

#### 3.1 数据平面执行模型

每个ReFlex服务器线程使用特定的核心来完成包的接受和传送，核心包括一个带直接和互斥的网络队列对和一个NVMe队列对来完成Flash的提交/完成命令。

Figure 2 展现的是一个ReFlex服务器线程处理一个到来的Flash读（或写）请求的执行模型。首先，NIC接收到了一个网络包并将它通过DMA递交给一个网络栈为其预先分配好的内存块（1）。ReFlex线程对接收到的描述器环进行轮询并通过以太网驱动和网络栈（例如TCP/IP）来处理包，生成表示新消息可用的事件条件（2）。同样的线程使用libix，一个和Linux libevent类似的库来处理事件。它包括对解析消息、提取I/O请求，实现接入控制检查和其他的存储协议的服务器码之间的切换，这些系统请求的处理要在Flash读/写的系统请求之前提交（3）。线程接着切换到系统请求处理并开始I/O调度来为共享ReFlex服务器的所用租户实行SLO。一旦调度好，请求便会通过一个NVMe提交队列被提交到Flash设备上（4）。Flash设备执行读（写）I/O并通过DMA递交（取回）到一个预先分配的用户空间缓存中（7）。线程轮询完成队列（5）并提交一个完成事件（6）。事件回调铜鼓olibix执行并释放一个send系统调用（7）。最终，线程处理send系统调用通过网络栈来递交被请求的数据回发起者（8）。执行模型对每个网络消息和横跨多个网络消息的大型I/O都支持多个I/O队列。

执行模型为远程Flash请求实现了低延迟。它在不包括任何额外的中断和线程的调度的条件下，经过的两步请求处理之后便运行结束，首先是网络包的接受和Flash命令的提交（1-4），然后是Flash的完成和网络包的传递（5-8）。

运行至结束消除了延迟的多变性并提高了为请求处理的数据缓存的局部性。ReFlex的两步运行至结束模型避免了Flash请求的阻塞。ReFlex通过对网络包的到来和Flash的完成进行轮询避免了中断的间接开销。进一步地，ReFlex通过传递指针的方式，让NIC或Flash设备到用于DMA的缓存数据传输实现“零复制”。

除了更低时延这个好处之外，ReFlex还用两种方法提升了远程Flash请求的吞吐量。首先，它使用异步的I/O来掩盖Flash设备处理网络或其他请求的时延（50us或更多）。一旦请求被递交到了Flash设备（4），线程将会轮询先前发出请求的需要发出网络处理的NVMe的完成队列和需要进入网络处理的NIC接收队列。只要有事情做，线程不会空闲。第二，ReFlex采用适应性的请求分批来分摊间接开销并提升预先取回和指导的缓存效率。在低负载下，到来的包或者已完成的NVMe请求将不带任何延迟地被处理。当负载上升时，NIC接收和NVMe的完成队列被充满并且提供处理同时处理多个到来的包或批处理中多个完成请求的机会。批处理的大小将会随负载上升，不过它会被盖到64以避免过多的时延。不同于传统的批处理那样需要在时延和带宽之间权衡，适应性批处理实现了高吞吐和低时延的较好平衡。

ReFlex可以扩展到多个线程，每个使用一个特定的核心和分别的硬件队列对。线程只要在发出对NVMe命令队列的接入时才需要协调，这一它们能够渐渐地响应共享一个ReFlex服务器（见§3 .2）的多个租户尾时延和吞吐量的SLO。一个本地的控制平面定期监视所用ReFlex线程的负载和它们实现预定SLO的能力来增减ReFlex线程。当线程数量改变时，远程租户和网络连接将像【53】中那样被再平衡。再平衡更需要几毫秒并且不会导致丢包和乱序。

#### 3.2 QoS调度和隔离

QoS调度器允许ReFlex为在一个服务器上共享Flash设备的租户提供性能保证。一个租户就是一个需要考虑并且实行SLO的逻辑分片，一个SLO在特定的吞吐量和读/写比下指定了一个尾读延迟阈值。举例来说，一个租户可能注册一个在80%读比例下的50K IOPS，而只有200us的读尾延迟（95th 百分位数）。除了这种要求保证尾时延和吞吐量分配的时延（LC）敏感型租户，ReFlex也能够服务于尽力型（BE）租户，它们倾向于投机地使用任何未被分配或使用的Flash带宽并能够容忍更高的时延。一个租户的定义能够被上千个从不同的客户端机器运行的任何应用发出的网络连接所共享。一个应用能够使用多个租户来为不同的数据流来请求分别的SLO。

如Figure 1中所示，在Flash设备接入上实行一个SLO会被两个因素所复杂化。第一，设备可以支持的最大带宽（IOPS）取决于它能够看到的所有租户请求整体的读/写比。第二，读请求的尾时延既取决于整体的读/写比也取决于现在的带宽负载。因此，QoS调度器要求对Flash设备和突出IO操作的类型的全局可见性和整体负载控制权。我们使用一个请求开销模型来计量每个Flash的I/O对读时延的影响，并使用一个创新的调度算法来保证所有租户和控制平面线程的SLO。ReFlex不对工作负载假设任何的先验知识。

##### 3.2.1 请求模型

模型将读尾时延作为一个带权重的IOPS函数，这个函数的请求的开销（权重）取决于IO规模，类型（读 vs 写），和现在设备上的读/写请求比：

【【【【】】】】

开销是一个关于r的函数，因为一些Flash设备相比于99%或更低的为一些只读的负载（r=100%）提供更高的IOPS，如Figure 1中所示。因此，模型会在当设备负载是只读的时候调整读请求的开销。开销被表示为多个令牌，每个令牌代表一个4KB的随机读请求。在所有我们使用的Flash设备里，请求规模大于4KB时（比如，一个32KB的请求耗费至多8个连续的的4KB请求），开销随着请求的规模线性增长。开销对于4KB及以下的请求是一个常数，这些Flash设备看起来是以4KB为粒度操作。

我们为每个的部署在ReFlex服务器中Flash设备的类型校准了开销模型。首先，我们测量了在工作负载下的本地Flash设备中，不同读写比和请求规模（见Figure 1中4KB的例子）影响的尾时延vs吞吐量。因为写请求的开销取决于垃圾回收的频率和页擦除的事件，我们保守地采用随机写模式来激发最差情况。接下来，我们使用曲线拟合来生成C（IO类型，r）。这个模型能够在部署后被重新校准来评价因为Flash磨损带来的性能降级。

Figure 3展现了在三个不同的NVMe设备的时延vs带权重的IOPS（令牌）。设备A在Figure 1中国呢被科幻。C（write，r<100%）的值代表A，B，C拥分辨有10，20和16个令牌。这意味着写操作比读操作取决于设备能够贵10到20倍。对于设备A，当从所有租户的负载是只读的，4KB写请求的开销是半个令牌（简单来说，C(读，r-100%) = 1/2 令牌）。对于三个设备，我们的线性开销模型导致了所有读写负载分布和请求规模下尾时延vs负载类似的行为。这种一致的行为让ReFlex的调度器能够在服务多个租户时以不同的吞吐量要求和读写比率来管理SLO。尽管非线性的曲线拟合模型能够拟合的更好，线性模型的精度足够我们的使用，出于简单性考虑，我们倾向于使用它。

##### 3.2.2调度机制

QoS调度器为LC租户在开销模型下构建来维持tial时延和吞吐量的SLO，而允许BE租户以公平的形式使用任何空闲的吞吐量。

令牌管理：在给定尾时延的SLO下，ReFlex调度器以一个Flash设备能够支持的和最大权重IOPS相同的速率生成令牌。ReFlex在所有共享一个Flash设备的LC租户中施行最严格（最低）的时延SLO。比方说，在一个使用Figure 3a中展示的开销模型的Flash设备上要服务两个尾读时延SLO分别为500us和1ms的租户，调度器生成每秒420K令牌来实施500us的SLO。LC租户被确保能够有一个满足它们IOPS的SLO的令牌供应，权重同它们的SLO制定的读写比。比方说，假设4KB请求和一次写需要10个令牌的开销，一个租户用80%的读比例注册一个100K IOPS的SLO，会被保证以0.8(100K IOPS)(1 tokens/(I/O)+0.2(100K IIOPS)(10 tokens/(I/O)) = 280K tokens/sec。由调度器生成但并没有分配给LC租户的令牌，会被公平地分布给BE租户。调度器在它提交租户的请求到Flash设备上时，基于每请求的开销来使用一个租户的令牌。

调度算法：

每个ReFlex线程以每租户，每软件队列的方式将Flash请求入队。当线程在数据平面执行模型（3.1）中到了QoS调度的步骤，线程使用算法1来计算入队请求的带权开销并提交所有可进入的请求到Flash设备上，渐渐地消费每个租户的令牌。取决于线程的负载和批处理的因素，执行模型每0.5到100us就会进入调度阶段。控制平面和批的大小限制保证调度器调用的事件间隔不会超过5%的最严格SLO，频繁的调度对于避免过多的排队延迟和维持NVMe设备的较高使用率是很有必要的。

时延敏感租户：调度算法从服务LC租户开始。首先，调度器为每个LC租户生成令牌，基于他们的IOPS SLO和上一次调度器调用的时间。自从控制平面决定每个LC租户的带权重IOPS的保留是可接受的，调度器能够特别地提交LC租户所有队列中的请求到NVMe设备中。然而，因为流量很少是一致的而且租户可能发出相比为它们SLO保留的平均IOPS或多或少的IOPS，调度器会跟踪每个租户的令牌使用。我们允许LC租户暂时地超过他们的令牌分配来避免段时间内的排队。然而，一旦它们达到了令牌负收益阈值（NEG_LIMIT），我们通过限制速率限制LC租户来限制爆发规模。这个参数经验上被设置为-50个令牌来限制在一次爆发中昂贵的写请求数量。我们也在这个阈值达到时通知控制平面来检测需要重新协商的带不合理的SLO的租户。

消耗得比它们可用令牌要少的租户被允许积累这些令牌为以后所用。积累量被POS_LIMIT参数所限制。当达到阈值时，调度器会捐献其一大部分的（经验上是90%）令牌到全局令牌桶给BE用户使用。POS_LIMIT是在经验上被设置为LC租户在上三个调度轮次接收到的令牌量来适应短暂的爆发而不用进入负收益。

Best-effort 租户：调度器通过公平地赋予每个BE租户设备上未被分配的吞吐量来为BE租户生成令牌。未被分配的设备吞吐量和设备能支持的令牌速率相关，在实行最严格的LC时延SLO减去LC租户令牌速率（基于LC租户的IOPS SLO）。假设有N个BE租户，每个调度阶段，每个BE租户接收1/Nth个为分配的令牌速率乘以自从上一轮调度过去的时间。加入一个BE租户没有足够的令牌来提交它所有的入队请求，租户能够从全局令牌桶中要回令牌，这个桶是由拥有空余令牌的LC租户供应的。BE租户以round robin的顺序跨调度阶段进行调度，以提供对全局令牌桶的公平接入。加入租户有足够的令牌应付请求，调度器有条件地提交一个BE请求。速率限制BE流量对于实现LC SLO是及其重要的。因为调度轮回以一个较高的频率进行，一个特定的轮回可能只生成一部分令牌。当请求队列非空时，BE租户跨越多个调度轮回收集令牌。当一个BE租户的软件队列为空时，我们不允许令牌收集来租住空闲后的爆发。调度器的这个方面是被DRR所启发的。未被BE租户使用的令牌会被捐献给全局令牌桶给其他BE租户使用。为了避免大量聚集允许BE租户发起不受控制的爆发情况，我们会定期清空这个桶。

加入ReFlex服务器管理多于一个NVMe设备，我们

### 4. ReFlex实现

ReFlex包括三个组件：服务器，客户端和控制平面。它们的实现使用了开源代码库。

#### 4.1 ReFlex服务器

远程Flash服务器是ReFlex的主要成份。我们将它作为IX——一个开源数据平面操作系统的扩展实现。IX采用了处理器虚拟化的和NIC的多队列支持（通过Intel的DPDK驱动）来获得对Linux系统多核心和多网络队列的直接的和互斥的获取。这些资源被用于运行一个数据平面核心和任何在此之上的应用。最初的IX数据平面是为网络密集型的工作负载开发的，如内存内的键值存储。IX对到来的请求采用“运行完成”策略并用带边界的，适应性的批处理来对尾时延和吞吐量进行最优化。IX也在线程中区分连接，分别用核心和队列而不用同步或是大量相关流量来扩展。这些优化让IX成为ReFlexcheng的一个很好的起点。

ReFlex用以下的方式扩展了IX。首先，我们开发了一个NVMe驱动来利用Intel的SPDK来对接Flash设备，并且获得了对NVMe队列对互斥的接入能力。第二，我们实现了Figure2中的数据平面模型。在IX中，运行结束模型包括所有对一个对键值对存储的请求从一个包的接收到响应的传输的所有工作。在ReFlex直接应用这个单一“运行结束”模型将会对每次Flash获取造成阻塞。相对的，我们引入了一个两部模型，它能够在保持运行结束模型的效率的同时允许异步.的Flash获取。第一个“运行结束”步骤是从包的接收到Flash响应命令的提交而第二个步骤是从从Flash控制的完成到响应的传输。我们维护着一个最大为64的适应性批处理。第三，我们实现了QoS调度器作为第一个运行结束步骤的一部分。第四，我们引入了Table 1中显示的对于远程Flash获取所必要的系统调用和事件。原生的IX为开放和关闭连接，接收和发出网络消息，以及网络错误的管理等定义了系统调用和事件。我们引入了用来注册和注销租户，提交和完成NVMe读写命令和管理NVMe错误的系统调用和事件。最后，我们开发了ReFlex用户级别的服务器码来消化被数据平面递交的事件并发出系统调用来响应。Cookie的参数允许用户空间的服务器码去追踪请求并取回在事件通知之上的上下文。注意所有的事件和系统调用是在一个共享的内存数组中通信，不存在任何阻塞、中断或是线程调度。这也令系统调用在高负载下也能够利用批处理来降低负载。数据平面采用“零复制”的方式实现；读写数据的缓冲区在ReFlex的用户空间码中被说出实话，提供给读写系统调用的参数。它们在用户空间码被通知送出完成信号时被释放。

ReFlex服务器是用C写的，源码中拥有一下代码（SLOC）：490个用户级别服务器的SLOC，954个IX NVMe驱动的SLOC和628个包括QoS调度器的数据平面的SLOC。我们利用Intel的DPDK、SPDK和IX数据平面的代码，其中包括lwIP TCP栈。

多线程操作：ReFlex能够扩展至多个线程，每个采用独立的河西能和独立的网络/NVMe队列。我们通过在租户间区分线程来将负载并行化。所有线程的操作是独立的并且能够在为同步的状态下发生，除了某些在3.2.2中描述的QoS调度动作。特别的，线程需要偶尔同步来从LC租户那交换空余的令牌这样任何在任意线程的BE租户能够从未被使用的Flash带宽中获益。线程使用原子性的“读-改-写”操作来获取全局的令牌桶。通过让每个线程至少在一个调度轮异步地标记其完成，这个桶定期会重置。最后一个线程重制全局桶。这个方法能够避免锁的间接开销并且能够将QoS调度从线程中解耦出来。特别的，它允许线程以不同的频率实现调度，但仍然保持着公平和系统级的SLO。

安全模型：ReFlex服务器以租户和网络连接的粒度来实行介入控制表（ACL）策略。它会检查一个客户端是否有权利去开启一个针对特定租户的连接和租户是否拥有对NVMe命名空间（逻辑块的范围）读或者写的权限。这些检查能够被扩展为采用证书机制。

跟随着IX，ReFlex以保护态内核模式（guest ring 0）来运行它的数据平面，而高级服务器码则是如Figure 2 中所示运行在用户空间（ring 3）。任何解析远程请求或其他高级服务功能的可挖掘的bug不能够引起硬件控制的损失，也不能够影响数据平面的操作或是任何运行在同一个机器上的通常的Linux应用。这个方法允许ReFlex和其他Linux应用共享Flash设备。以Linux工作负载（核心态或用户态）对于Flash的接入能够通过ReFlex包含QoS调度器的保护部分所缓解。从QoS的角度看，Linux请求被看作是带有特定吞吐量和时延保证的时延敏感型请求。相比于在所有用户模式下运行的ReFlex，我们实现了虚拟的类似表现。从内核态到用户态的过渡的内在开销和一个主存获取的开销相似，能够被ReFlex提高的局部性弥补，这得益于其对运行结束和零复制的采用。

局限性：现在的服务器的实现有一些非基础的局限性，我们会子啊后续的版本中消除。首先，我们限制了每个租户只能使用一个ReFlex线程。因为ReFlex能够支持每个线程多达850K的远程IOPS并且一个应用能够利用多个租户来获取同样的数据，这对任何应用来说都不是一个重要的性能瓶颈。在未来，如果对跨线程的租户的总体需求超过了一个线程的吞吐量，我们会对其连接进行负载均衡。第二，我们已经在ReFlex数据平面实现了一个网络协议，一个泛在的TCP/IP。因为TCP/IP时数据中心最重量级的协议，这是一个定义ReFlex下界的保守选择。尾时延和吞吐量都会在我们实现UDP或者其他轻量级的传输协议时提升。最后，ReFlex现在的能够 不以任何顺序保证地服务远程读写请求，这是在在网络协议强制的排序之上（例如，TCP连接内的排序）。未来，我们会支持能够被用于强制排序和像原子事务那样构建高级抽象的栅栏操作。

#### 4.2 ReFlex客户端

应用能够使用一系列客户端通过网络接入ReFlex服务器。我们实现了两个替代方案来代表性能的极端点。

第一个实现是一个用户级别的库（536 SLOC），和为memcache键值存储的二进制协议写的客户端库类似。这个库允许应用开启和ReFlex的TCP协议并传递读写请求到逻辑块。这个客户端方法避免了其机器上的文件系统和操作系统的块层带来的间接开销。然而，客户端仍然受限于其操作系统网络层任何的时延或吞吐量的低效率。

为了支持传统的客户端应用，我们也实现了一个能将ReFlex服务器看作是Linux块设备（845 SLOC）的远程块设备。驱动将传统的Linux块I/O（bio）请求翻译成上文提到的用户级别库的ReFlex接入。驱动实现了多队列（blk-mq）内核API并且为每个核心支持一个硬件上下文来开启随核心的线性增长。对于每个硬件上下文，驱动开启了一个到ReFlex的套接字并且创建一个内核线程来接收和完成到来的响应。为了最小化时延，驱动直接在不联合的情况下发出每个块到服务器，这时ReFlex请求的间接开销很小（每4KB请求38比特）并且NVMe设备的带宽在使用大于4KB请求的情况下变动很小。在4KB时，Linux的TCP栈能够支持每个线程多达70K的消息并且因此驱动需要执行至少4个线程（或者6个对于提升的时延）来充分使用一个10GbE的接口，正如§5.6中所示。

#### 4.3 ReFlex控制平面

ReFlex的控制平面包括两个组件，一个运行在每个ReFlex服务器上的本地组件和一个全局的。我们目前实现了前者。

本地控制平面负责以下的动作。首先，当新的LC租户注册，控制平面决定这个租户是否可进入和哪个服务器线程来绑定给它。它使用了最严格的来自所有LC租户的时延SLO并且用每个设备的吞吐量-时延特征来车人新租户的SLO能不能在不与现存租户的SLO冲突的前提下满足。当一个租户注册或是终止，控制平面会为LC和BE租户重新计算令牌的生成速率。控制平面会在某个LC租户持续地爆发出它的SLO分配是通过通知其他租户重新协商来进行干涉。第二，本地控制平面会见识请求时延和线程负载。假如时延和负载都很高，它会分配资源给新增的线程并在租户之间再平衡。加入负载很低，它会撤回某些线程和资源的分配，将它们返回给Linux系统通常使用。上一个功能是IX控制平面的一个衍生，它能够动态地给IX数据平面使用线程的数量和时钟频率正确的大小，而不会有任何的丢包和乱序。最后，控制平面定期会校准请求开销模型并决定每个Flash设备的吞吐量-时延特性。

在未来的工作中，我们会开发一个全局的控制平面来跨数据中心集群管理远程的Flash资源，并且最优化Flash能力和IOPS的的分配。比方说，全局的控制平面应该尝试去用类似的尾时延要求来互相搭配，这样对某个租户的严格要求不会限制其他租户的IOPS。全局的控制平面应该也能为跨ReFlex服务器的应用维持全局的时延和吞吐量的SLO。

### 5. 评价

#### 5.1 实验方法论

硬件设置：我们的实验设置包括采用英特尔至强CPU E5-2630处理器（Sandy Bridge EP）的标识服务器和客户端机器，其中12个物理内核跨两个运行2.3 GHz和64GB DRAM的插槽。这些机器使用通过Arista 7050S-64交换机连接的Intel 82599ES 10GbE NIC。它们使用4.4 Linux内核运行Ubuntu LTS 16.04。服务器机器装有PCIe连接的闪存设备，预先连续写入整个地址空间，然后进行一系列随机写入以达到稳定状态性能。我们使用三种不同的闪存设备测试了ReFlex，其请求成本模型如图3所示。我们使用设备A显示了ReFlex的结果，因为它实现了最高的原始IOPS，对于只读工作负载高达1M IOPS（请参见图1）。对于所有实验，我们禁用电源管理并以最高频率运行CPU内核，以确保结果的保真度。 NIC配置为启用巨型帧，禁用大容量接收卸载（LRO）和通用接收卸载（GRO）。 LRO和GRO会扭曲卸载的延迟，因为收到的数据包有时会被缓冲，而不是直接传递给内核。我们以20μs的间隔启用中断合并。
客户端：我们在大多数实验中使用基于Linux的客户端。我们扩展了mutilate负载生成器以使用我们的用户级客户端库并向Re-Flex发出读/写请求。mutilate通过多个机器协调大量客户端线程以产生期望的吞吐量，而单独的未加载客户端通过一次发出一个请求来测量延迟。为了降低客户端性能开销，我们还评估了ReFlex的每个内核的未加载延迟和峰值IOPS，以及在IX数据平台上运行类似负载生成器的客户端，与Linux网络相比，延迟和吞吐量显着降低叠加。
I/O大小：我们在大多数实验中发出4KB读写请求。由于我们使用10GbE网络基础架构，因此发出4KB IOPS的客户端可以在重新启动服务器的NIC之前充分饱和NVMe闪存设备（1M IOPS峰值）。因此，我们在一些实验中使用1KB请求来强调ReFlex服务器的IOPS。现代数据中心包括40GbE网络基础设施，未来的数据中心可能会部署100GbE。这两项技术都将消除这一瓶颈。
基线：我们对比了ReFlex远程访问的性能和使用SPDK发出本地访问闪存设备的性能。 SPDK提供了我们期望的最佳本地性能，因为它使软件直接访问NVMe队列，而无需通过Linux文件系统或块设备层。我们还将ReFlex与两种基于软件的远程Flash访问方案进行了比较：1）Linux iSCSI系统和2）轻量级远程存储服务器，通过使用libevent有效处理每个线程的多个连接，从而最大化Linux性能并使用libaio重叠进行通信和计算。我们无法访问硬件加速的远程Flash环境，但我们将其与NVMe over RDMA Fabrics公开演示中引用的结果进行了比较。在§5.4中，我们评估了NVMe设备在没有基于软件的QoS调度器（如ReFlex中的调度器）时共享的性能问题。

#### 5.2 未加载时延

我们首先测量Flash访问的卸载延迟。表2显示了用队列深度1发布的4KB随机读取和写入请求的平均和第95百分位延迟。远程访问包括客户端和服务器中的往返网络开销。通过iSCS进行远程访问由于客户端和服务器端的重量级协议处理（包括套接字，SCSI和应用程序缓冲区之间的数据复制），I读取请求的延迟增加了2.8倍。 libaio-libevent远程Flash服务器比iSCSI快得多，但Linux网络和存储栈仍然增加了超过100μs的平均和尾部延迟。 ReFlex的数据面板执行模型为本地闪存延迟（IX客户端）增加了21μs。在113μs的尾部读取能力下，ReFlex服务器的性能和许多（本地）NVMe设备的接近。由于闪存设备上的DRAM缓冲，未加载的写入延迟低于读取延迟。因此，对于写入I / O，iSCSI和libaio-libevent的开销更为重要。 ReFlex的性能优于iSCSI和libaio-libevent，为本地写入延迟增加20μs（IX客户端）。将ReFlex延迟结果与Linux和IX客户端进行比较表明，对于低延迟远程Flash访问，优化客户端也很重要。
采用更高吞吐量的40GbE Chelsio NIC（较低的传输延迟4KB）和3.6GHz Haswell CPU（2.3GHz Sandy Bridge CPU）测量，NVMe over Fabrics提供了极低的延迟开销（8μs）。使用ReFlex的远程Flash延迟包括完整的TCP / IP堆栈和允许多个客户端连接到Flash服务器的QoS调度程序。 ReFlex可能会受益于Chelsio NIC中的TCP卸载。

#### 5.3 吞吐量和CPU资源开销

图4绘制了针对1KB只读请求的尾部延迟（第95百分位），作为吞吐量（IOPS）的函数。即使是使用SPDK对Flash进行本地访问，也需要两个内核来满足Flash设备的1M IOPS。单个内核可以在本地Flash上支持高达870K的IOPS。 ReFlex通过一个用于网络和存储处理的内核实现了高达850K的IOPS。凭借两个内核，ReFlex在Flash上达到1M IOPS，与本地访问相比，延迟开销可以忽略不计。相比之下，由于请求处理的计算强度更高，libaio-libevent服务器仅实现了75K IOPS /内核并且延迟更高。该服务器需要10倍以上的CPU核才能实现ReFlex的吞吐量。据称，硬件加速的NVMe覆盖范围可以在3.6GHz Haswell内核的20％利用率下达到460K IOPS。
在高负载下，ReFlex线程在TCP / IP处理上花费约20％的执行时间。因此，再加上较轻的网络协议，ReFlex可以提供更高的吞吐量。花在QoS调度上的时间在2％到8％之间变化，取决于服务的租户数量。
ReFlex能够在不影响尾部延迟的情况下为数百万个IOPS提供数百万IOPS，这对于使远程Flash在数据中心实用且具有成本效益非常重要。为了将每个核心的IOPS看得更清楚，假设我们在Intel最新的Broadwell或Skylake类CPU上部署ReFlex。改进的内核性能可能允许ReFlex达到1M IOPS /内核。假设每个CPU插槽有20个核心，ReFlex将能够使用2.5插槽服务器计算容量的2.5％共享1M IOPS闪存设备。或者，使用4个闪存设备，ReFlex将需要8％的服务器计算容量来饱和具有4KB I/O的100GbE链路。

#### 5.4 QoS和隔离的性能

我们现在使用具有不同SLO的多个租户评估QoS调度程序。以下实验使用单个ReFlex线程。我们在§5.5中评估多核可扩展性。我们首先考虑情景1（图5a-5b），其中两个时延敏感型（A，B）和两个尽力而为型（C，D）租户共享一个ReFlex服务器。 A和B都需要第95百分位读取延迟500μs。租户A需要100％读取下的120K IOPS，而B在80％读取时需要70K IOPS。 C和D分别是95％和25％读取负载的BE租户。为了保证500μs以下的尾部读取延迟SLO，我们的闪存设备可以支持高达420K的加权IOPS。因此，QoS调度器产生420K令牌/秒。LC租户A收到120K令牌/秒，而租户B收到196K令牌/秒  = 【】【】【】。因此，这两名租户共同保留了75％的设备吞吐量，为BE租户留下25％的令牌。图5显示了禁用和启用QoS调度程序的每个租户的尾部延迟和IOPS。如果没有QoS调度，由于读/写干扰，所有租户的尾部读取延迟将高于2毫秒。租户B也在其SLO吞吐量下运行。在QoS调度允许的情况下，两个LC吞吐量（图5a）均以满足BE吞吐量为代价满足延迟和吞吐量SLO（图5b）。 BE租户C和D分别获得未分配公平份额（每个为52K令牌/秒），但由于其写入I/O的写入成本比写入成本高10倍，因此D的IOPS低于C。
场景2使用与场景1相同的租户以及相同的SLO。但是，时延敏感型租户B只能发出45K IOPS，而不是其SLO中保留的70K。 BE租户现在可以获得更高的吞吐量（图5c - 5d），因为他们获得租户B未使用的令牌，以及未分配给LC租户的令牌。 BE租户的循环服务确保公平访问未使用的令牌。
虽然这些场景仅涉及4个租户，但它们足以显示远程Flash访问的QoS调度需求，超出了硬件所提供的范围。我们的QoS调度程序可以保证SLO，同时为尽力而为的租户提供节约和公平的服务。

#### 5.5 可伸缩性

我们现在评估ReFlex如何在核心，租户和连接的维度中扩展。
内核：我们运行最多12个内核（每个套接字6个内核）的ReFlex，以测试调度程序的多核可伸缩性。每个线程管理单个LC租户，SLO为20K IOPS（90％读取，4KB请求），读取延迟最长为2毫秒（第95百分位）。 2ms等待时间的SLO使得我们的闪存设备能够在SLO由于太多的写入干扰而不再被允许之前为多达12个这样的租户提供服务。两个ReFlex线程也分别服务于BE租户（80％读取，4KB）。图6a显示了LC租户聚合IOPS的线性增长，因为我们调整了调度程序中没有任何缩放瓶颈的核心（租户）数量。同时，由于设备上的备用带宽较少，速率限制导致总体BE IOPS降低。尽管未在图6中显示，但所有LC租户的尾部读取延迟仍低于2ms SLO。由于两个BE租户被允许发出尽可能多的请求，因此当没有注册LC租户时，总令牌使用率（带有标记在辅助y轴上的绿线）很高。只要第一个LC租户注册，调度程序就会将令牌速率限制为570K令牌/秒，以强制执行2ms SLO。随着我们扩展核心数量，令牌使用率仍保持在此水平，因为ReFlex在不违反SLO的情况下会让闪存设备饱和。
租户：我们评估在租户管理成为性能瓶颈之前，每个ReFlex线程可以提供的租户数量。每个租户使用单个连接发出100个1KB读取IOPS。在此实验中，每个连接的低IOPS是必要的，以避免在达到租户缩放限制之前使Flash设备饱和。图6b显示，单个ReFlex核心最多可以服务2,500个租户，而2个ReFlex核心服务5,000个租户，而4核ReFlex服务器接近支持10K租户，此时我们接近1M读取IOPS限制闪存设备。随着我们缩减每个线程的租户数量，尾时延可能会随着QoS调度频率的降低而增加。这种情况由ReFlex控制平面检测，该平面分配更多内核并根据需要重新平衡租户（请参阅第4.3节）。
 TCP连接：租户可用于跟踪使用多个客户机和线程的应用程序的QoS要求。因此，知道ReFlex服务器可以处理多少个TCP连接很重要。图6c绘出了在缩放与租户关联的TCP连接数量时单个ReFlex线程的吞吐量。每个连接100 IOPS时，单个ReFlex线程可支持高达5K的连接。由于主存访问，TCP连接状态不再适合最后一级缓存，并且TCP / IP处理速度减慢，所以性能会逐渐降低。这与IX的连接缩放行为相似，在较小消息（64B对1KB）的实验中，该连接在10K连接处饱和，从而触发更少的失败。凭借1K IOPS /连接，ReFlex内核接近峰值带宽，通过850个连接实现780K IOPS。由于缓存压力较高，峰值带宽低于§5.3中的850K IOPS。

#### 5.6 Linux应用性能

我们现在使用ReFlex的远程块设备驱动程序来评估传统Linux应用程序的性能。我们比较了本地NVMe设备驱动程序和Linux iSCSI远程块I/O的性能。我们展示了以下应用程序的结果：灵活的I/O测试器（FIO），FlashX图形分析框架和Facebook的RocksDB键值存储。
FIO：图7a显示了FIO发布4KB随机读取的延迟吞吐量曲线，队列深度最高为64。我们需要多个FIO线程才能达到最大吞吐量：带有本地NVMe驱动程序的5个线程，带有iSCSI的3个线程以及6个与ReFlex块驱动程序。正如预期的那样，ReFlex在客户端和服务器都饱和10GbE网络接口时停止扩展。但是，由于ReFlex之上的FIO可以线性扩展到6个线程，我们预计它将能够在给定更高带宽的网络链接的情况下匹配本地吞吐量。在这个实验中ReFlex的更高延迟是由于Linux块和网络层的客户端的开销。不过，ReFlex的吞吐量比iSCSI高4倍，而尾部和平均延迟则低2倍。我们在§5.3中对优化客户端的ReFlex进行了评估。
FlashX：我们使用FlashX，这是一个图形处理框架，它使用SAFS用户空间文件系统有效存储和检索来自Flash的顶点和边缘数据。我们在SNAP的SOC-LiveJournal1社交网络图上执行四个图基准测试，包括弱连接组件（WCC），pagerank（PR），广度优先搜索（BFS）和强连通组件（SCC）。该图包含4.8M顶点和68.9M边，我们通过iSCSI或ReFlex将其存储在本地块设备或远程块设备上。图7b显示了访问远程Flash对端到端应用程序执行时间的影响。与本地Flash的性能相比，ReFlex只引入了小幅下滑，WCC为1％，BFS为3.8％。相比之下，对于BFS和SCCC，iSCSI的性能降低了15％。
RocksDB：最后，我们使用RocksDB通过ReFlex评估Flash上的关键值存储性能。我们在NVMe块设备上安装ext4文件系统，并通过ReFlex或iSCSI将其为本地或远程安装。我们将RocksDB的数据库及其预写日志放在Flash上。我们使用db bench，这是RocksDB提供的基准测试工具生成工作负载。我们使用cgroups来限制内存并减少Linux页面缓存的影响，从而通过对43GB数据库的简短实验来训练闪存。我们首先使用批量加载（BL）例程填充数据库，然后执行随机读取（RR）和读写（RwW）基准。图7c显示了与本地闪存相比，RocksDB在iSCSI和ReFlex上的端到端执行时间减缓。对于写密集型BL测试，本地和远程的性能几乎相同，因为Flash本身限制了IOPS。对于RR和RwW，iSCSI分别显示了32％和27％的减速，而ReFlex则将性能降低了不到4％。

### 6. 讨论

目前的Flash硬件有两个限制与ReFlex特别相关。
读/写干扰：写操作对并发读请求的尾部延迟有很大的影响。我们的调度程序使用请求成本模型来避免超过Flash设备的当前读/写比率的延迟-吞吐量能力。但是，我们仍然限于在第95百分位执行尾读取延迟SLO。更严格的SLO（例如99或99.9百分位数）很难在现有的闪存设备上执行，而不会显着降低IOPS，因为读取频繁停滞在写入，垃圾收集或耗损均衡任务之后。未来的闪存设备应该限制读/写干扰，除了平均值之外，还要针对尾部行为。例如，在写入Flash页面之前，Flash翻译层（FTL）总是可以读出并缓冲Flash页面[。
请求调度的硬件支持：现有Flash设备使用简单的round-robin仲裁来调度来自不同NVMe硬件队列的请求。为了保证SLO，ReFlex必须使用软件调度程序来实现速率限制和优先级。 NVMe规范定义了一个加权round-robin仲裁器，但它并未被我们所知的任何Flash设备所实现。这个仲裁器将允许ReFlex线程向具有适当权重的优先级的硬件队列提交请求，从而消除了在软件租户之间执行优先级的需要。 ReFlex仍然会在软件中实施速率限制，因为它必须在不同的读写比率下管理设备延迟吞吐量特性，抵御SLO冲突（LC租户的长时间爆发），并支持可能超过数量的租户数量的硬件队列。

### 7. 相关工作

我们讨论有关存储QoS和高性能网络堆栈的相关工作。 §2中讨论了远程访问Flash的其他方法。
存储QoS：以前的工作已经广泛研究了服务质量和共享存储的公平性。基于时间片的I/O调度器（如Argon，CFQ和FIOS）为租户提供专用设备访问，以实现公平性。这种方法可能导致响应性差，并且Flash的时间片可能并不总是公平的，因为背景任务（即垃圾收集）会影响设备性能。
相比之下，基于公平排队的解决方案交叉所有租户的请求。最初的加权公平排队调度程序已经成功地从网络调整到存储I/O，并支持再排序，调节和/或分批请求以利用设备并行。我们的I / O调度器类似于Deficit Round Robin，因为租户只要有需求它们每轮都会累积令牌。 Zygaria和AQuA也采用令牌桶方式为实时租户提供服务
同时为尽力而为的流量提供备用设备带宽，但它们仅提供吞吐量保证，而ReFlex还实施尾时延SLO。
尾时延SLO：PriorityMeister通过使用类似于ReFlex的令牌桶机制调整对共享网络和存储资源的访问，从而即使在第99.99个百分点也能提供尾等待时间保证。与ReFlex不同的是，调度程序将工作负载分配给延迟关键租户的不同优先级。 Cake使用反馈控制器来执行尾延迟SLO，但仅支持单个延迟严重的租户。Avatar依靠反馈而不是设备特定的性能模型来控制磁盘上的尾部延迟。
特定于Flash的挑战：许多专门为Flash设计的I/O调度程序使用请求成本模型来计算读/写干扰。 FIOS是第一批解决闪存写入干扰并使用时间片提供公平性的计划之一。 FlashFQ是一种基于虚拟时间的调度程序，通过节制调度和I/O预测来提高公平性和响应性。 Libra是一种I/O调度框架，用于分配每租户吞吐量预留量，并使用虚拟IOPS度量来捕获原始IOPS和带宽之间的非线性。尽管FIOS，FlashFQ和Libra都分配了I/O成本，但它们的成本模型并不一定会捕获请求对并发I/O的尾延迟的影响，因为这些调度器专为公平性和吞吐量保证而非延迟QoS设计。
高性能网络：ReFlex利用IX数据平台实现高性能网络。诸如mTCP ，Sandstorm和OpenOnload等异构网络堆栈在用户空间中应用了类似的技术，以实现高吞吐量和/或低延迟网络。

### 8. 结论

我们描述了ReFlex，一个在商业网络上为远程Flash获取的软件系统。ReFlex使用一个紧耦合并减少间接开销的网络和存储处理的数据平面设计。这令系统能够支撑至多850K的IOPS， 然而却仅仅只比获取本地Flash多花21us。ReFlex使用的QoS调度器能够为数千个远程客户端执行可忽略的时延和吞吐服务级别目标（SLO）。ReFlex允许应用灵活地在机器间分配远程Flash却又保持近乎本地Flash的性能。

_References:_

[1] Klimovic, A., Litz, H., & Kozyrakis, C. (2017). Reflex: remote flash ≈ local flash. ACM SIGOPS Operating Systems Review, 51(2), 345-359.
[2] http://translate.google.cn
